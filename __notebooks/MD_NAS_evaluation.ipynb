{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /localscratch/2183901/matplotlib-oqrrumcj because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mautushid/Cowsformer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ultralytics import NAS\n",
    "os.chdir(\"..\")\n",
    "ROOT = os.getcwd()\n",
    "print(ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TORCH_HOME'] = '/home/mautushid/.torch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
=======
>>>>>>> 75d00b381e3a5fcb12b0505ee91bac59a8e8cc0a
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-03 14:26:37] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The console stream is logged into /home/mautushid/sg_logs/console.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[2024-03-01 15:45:20] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n",
      "[2024-03-01 15:45:21] WARNING - __init__.py - Failed to import pytorch_quantization\n",
      "[2024-03-01 15:45:30] WARNING - calibrator.py - Failed to import pytorch_quantization\n",
      "[2024-03-01 15:45:30] WARNING - export.py - Failed to import pytorch_quantization\n",
      "[2024-03-01 15:45:30] WARNING - selective_quantization_utils.py - Failed to import pytorch_quantization\n"
=======
      "[2024-03-03 14:26:37] WARNING - __init__.py - Failed to import pytorch_quantization\n",
      "[2024-03-03 14:26:38] WARNING - redirects.py - NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "[2024-03-03 14:26:39] WARNING - calibrator.py - Failed to import pytorch_quantization\n",
      "[2024-03-03 14:26:39] WARNING - export.py - Failed to import pytorch_quantization\n",
      "[2024-03-03 14:26:39] WARNING - selective_quantization_utils.py - Failed to import pytorch_quantization\n",
      "[2024-03-03 14:26:39] WARNING - env_sanity_check.py - \u001b[31mFailed to verify operating system: Deci officially supports only Linux kernels. Some features may not work as expected.\u001b[0m\n"
>>>>>>> 75d00b381e3a5fcb12b0505ee91bac59a8e8cc0a
     ]
    }
   ],
   "source": [
    "from models.nas import *\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 3,
>>>>>>> 75d00b381e3a5fcb12b0505ee91bac59a8e8cc0a
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[2024-03-01 15:45:32] INFO - checkpoint_utils.py - License Notification: YOLO-NAS pre-trained weights are subjected to the specific license terms and conditions detailed in \n",
      "https://github.com/Deci-AI/super-gradients/blob/master/LICENSE.YOLONAS.md\n",
      "By downloading the pre-trained weight files you agree to comply with these terms.\n",
      "[2024-03-01 15:45:32] INFO - checkpoint_utils.py - Successfully loaded pretrained weights for architecture yolo_nas_l\n"
=======
      "[2024-03-03 14:26:39] INFO - checkpoint_utils.py - License Notification: YOLO-NAS pre-trained weights are subjected to the specific license terms and conditions detailed in \n",
      "https://github.com/Deci-AI/super-gradients/blob/master/LICENSE.YOLONAS.md\n",
      "By downloading the pre-trained weight files you agree to comply with these terms.\n",
      "[2024-03-03 14:26:40] INFO - checkpoint_utils.py - Successfully loaded pretrained weights for architecture yolo_nas_l\n"
>>>>>>> 75d00b381e3a5fcb12b0505ee91bac59a8e8cc0a
     ]
    }
   ],
   "source": [
    "### class inits and other inputs\n",
    "\n",
    "path_model = 'yolo_nas_l' \n",
<<<<<<< HEAD
    "dir_train = \"/home/mautushid/Cowsformer/data/cow200/yolov5/train\"\n",
    "dir_val = \"/home/mautushid/Cowsformer/data/cow200/yolov5/val\"\n",
    "dir_test = \"/home/mautushid/Cowsformer/data/cow200/yolov5/test/test\"\n",
=======
    "dir_train = \"/Users/mautushid/github/Cowsformer/data/cow200/yolov5/train\"\n",
    "dir_val = \"/Users/mautushid/github/Cowsformer/data/cow200/yolov5/val\"\n",
    "dir_test = \"/Users/mautushid/github/Cowsformer/data/cow200/yolov5/test_old\"\n",
>>>>>>> 75d00b381e3a5fcb12b0505ee91bac59a8e8cc0a
    "name_task = \"cow200\"\n",
    "data_yaml_path = \"/home/mautushid/Cowsformer/data/cow200/yolov5/data.yaml\"\n",
    "\n",
<<<<<<< HEAD
    "#data_yaml_path = \"/home/mautushid/Cowsformer/data/cow200/yolov5/data.yaml\"\n",
    "finetuned_model_path = \"/home/mautushid/Cowsformer/checkpoints_cow200/n200_yolo_nas_l_i12/RUN_20240224_060546_568311/ckpt_best.pth\"\n",
=======
    "data_yaml_path = \"/Users/mautushid/github/Cowsformer/data/cow200/yolov5/data.yaml\"\n",
    "finetuned_model_path = \"/Users/mautushid/github/Cowsformer/__notebooks/lms_checkpoints/yolo_l_100/n100_yolo_na_i2_exp_yolo_nas_l_100_2/ckpt_best.pth\"\n",
>>>>>>> 75d00b381e3a5fcb12b0505ee91bac59a8e8cc0a
    "\n",
    "### Creating instance of Niche_YOLO_NAS class\n",
    "my_nas = Niche_YOLO_NAS(path_model, dir_train, dir_val, dir_test, name_task)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model /home/mautushid/Cowsformer/checkpoints_cow200/n200_yolo_nas_l_i12/RUN_20240224_060546_568311/ckpt_best.pth loaded\n"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-03 14:26:41] INFO - checkpoint_utils.py - Successfully loaded model weights from /Users/mautushid/github/Cowsformer/__notebooks/lms_checkpoints/yolo_l_100/n100_yolo_na_i2_exp_yolo_nas_l_100_2/ckpt_best.pth EMA checkpoint.\n"
     ]
    }
   ],
   "source": [
    "### load finetuned model\n",
    "model = my_nas.load(path_model,finetuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mautushid/miniconda3/envs/myenv/lib/python3.9/site-packages/numpy/lib/arraypad.py:487: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x = np.array(x)\n",
      "/Users/mautushid/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "[2024-03-03 14:27:06] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:06] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:07] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:07] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:07] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:08] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:08] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:09] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:09] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:10] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:10] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:11] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:11] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:11] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:12] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:12] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:13] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:13] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:14] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:14] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:14] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:15] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:15] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:16] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:16] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:17] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:17] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:18] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:18] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:18] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:19] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:19] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:20] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:20] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:21] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:21] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:21] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:22] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:22] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:23] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:23] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:24] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:24] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:24] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2024-03-03 14:27:25] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"
>>>>>>> 75d00b381e3a5fcb12b0505ee91bac59a8e8cc0a
     ]
    }
   ],
   "source": [
    "import supervision as sv\n",
    "\n",
    "ds = sv.DetectionDataset.from_yolo(\n",
    "    images_directory_path=\"/Users/mautushid/github/Cowsformer/data/cow200/yolov5/test_old/images\",\n",
    "    annotations_directory_path=\"/Users/mautushid/github/Cowsformer/data/cow200/yolov5/test_old/labels\",\n",
    "    data_yaml_path=\"/Users/mautushid/github/Cowsformer/data/cow200/yolov5/data.yaml\",\n",
    "    force_masks=False\n",
    ")\n",
    "CONFIDENCE_TRESHOLD = 0.5\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "for image_name, image in ds.images.items():\n",
    "    result = list(model.predict(\n",
    "        image, conf=CONFIDENCE_TRESHOLD))[0]\n",
    "    detections = sv.Detections(\n",
    "        xyxy=result.prediction.bboxes_xyxy,\n",
    "        #xyxy=result.prediction.bboxes_xywh,\n",
    "        confidence=result.prediction.confidence,\n",
    "        class_id=result.prediction.labels.astype(int)\n",
    "    )\n",
    "    predictions[image_name] = detections"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 57,
>>>>>>> 75d00b381e3a5fcb12b0505ee91bac59a8e8cc0a
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[2024-03-01 15:45:35] INFO - detection_dataset.py - Dataset Initialization in progress. `cache_annotations=True` causes the process to take longer due to full dataset indexing.\n",
      "Indexing dataset annotations: 100%|██████████| 50/50 [00:00<00:00, 178.36it/s]\n",
      "Testing:  33%|███▎      | 1/3 [00:02<00:05,  2.62s/it]"
=======
      "All predictions have been written to path/to/output/directory\n"
>>>>>>> 75d00b381e3a5fcb12b0505ee91bac59a8e8cc0a
     ]
    }
   ],
   "source": [
    "import os\n",
    "'''\n",
    "# Assuming `predictions` is your dictionary of results as shown above\n",
    "# And assuming `output_directory` is the path to the directory where you want to save the .txt files\n",
    "output_directory = 'path/to/output/directory'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Iterate over the predictions and write them to files\n",
    "for image_name, detections in predictions.items():\n",
    "    # Prepare the output file path\n",
    "    base_filename = os.path.splitext(image_name)[0]\n",
    "    output_file_path = os.path.join(output_directory, base_filename + '.txt')\n",
    "\n",
    "    # Open the file and write the detections\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for bbox, conf, class_id in zip(detections.xyxy, detections.confidence, detections.class_id):\n",
    "            # Create a string for each detection\n",
    "            # Format: class_id x_min y_min x_max y_max confidence\n",
    "            detection_str = f\"{class_id} {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]} {conf}\\n\"\n",
    "            # Write to file\n",
    "            file.write(detection_str)\n",
    "\n",
    "print(f\"All predictions have been written to {output_directory}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# The following methods are not used in the current project\n",
    "############################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "data": {
      "text/plain": [
       "{'Precision@0.50': 0.0,\n",
       " 'Recall@0.50': 0.0,\n",
       " 'mAP@0.50': 0.0,\n",
       " 'F1@0.50': 0.0,\n",
       " 'Best_score_threshold': 0.0,\n",
       " 'Precision@0.50:0.95': 0.0,\n",
       " 'Recall@0.50:0.95': 0.0,\n",
       " 'mAP@0.50:0.95': 0.0,\n",
       " 'F1@0.50:0.95': 0.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_old\n",
      "test_old/output\n"
     ]
    }
   ],
   "source": [
    "from API import*\n",
    "api = YOLO_API(\"/Users/mautushid/github/Cowsformer/data/cow200/yolov5\") \n",
    "lbs = api.get_gt_detections(\n",
    "    \"test_old\")\n",
    "pre = api.get_pred_detections(\n",
    "    \"test_old/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import from_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = from_sv(pre, lbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
>>>>>>> 75d00b381e3a5fcb12b0505ee91bac59a8e8cc0a
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "\r",
      "Testing: 100%|██████████| 3/3 [00:02<00:00,  1.31it/s]\r",
      "Testing: 100%|██████████| 3/3 [00:02<00:00,  1.04it/s]\n"
=======
      "{'map5095': 0.7202700813553288, 'map50': 0.9558591094378901, 'precision': 0.854066985645933, 'recall': 0.9444444444444444, 'f1': 0.8969849246231155, 'n_all': 378, 'n_fn': 21, 'n_fp': 61}\n"
>>>>>>> 75d00b381e3a5fcb12b0505ee91bac59a8e8cc0a
     ]
    }
   ],
   "source": [
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_nas.get_evaluation_matrix(best_model, data_yaml_path)"
=======
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   map5095     map50  precision    recall        f1  n_all  n_fn  n_fp   \n",
      "0  0.72027  0.955859   0.854067  0.944444  0.896985    378    21    61  \\\n",
      "\n",
      "   config       model    n  \n",
      "0  cow200  yolo_nas_l  200  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def make_table_row(EvalOutput, dataset_name, model_name, data_size):\n",
    "    # Combine EvalOutput with the additional information\n",
    "    combined_dict = {\n",
    "        **EvalOutput,  # Unpack the EvalOutput dictionary\n",
    "        'config': dataset_name,  # Add dataset_name as 'config'\n",
    "        'model': model_name,  # Add model_name as 'model'\n",
    "        'n': data_size  # Add data_size as 'n'\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame from the combined dictionary\n",
    "    # The index=[0] makes the dictionary values to be considered as a single row\n",
    "    df = pd.DataFrame([combined_dict])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "dataset_name = \"cow200\"\n",
    "model_name = \"yolo_nas_l\"\n",
    "data_size = \"200\"\n",
    "\n",
    "df = make_table_row(op, dataset_name, model_name, data_size)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>map5095</th>\n",
       "      <th>map50</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>n_all</th>\n",
       "      <th>n_fn</th>\n",
       "      <th>n_fp</th>\n",
       "      <th>config</th>\n",
       "      <th>model</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.72027</td>\n",
       "      <td>0.955859</td>\n",
       "      <td>0.854067</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.896985</td>\n",
       "      <td>378</td>\n",
       "      <td>21</td>\n",
       "      <td>61</td>\n",
       "      <td>cow200</td>\n",
       "      <td>yolo_nas_l</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   map5095     map50  precision    recall        f1  n_all  n_fn  n_fp   \n",
       "0  0.72027  0.955859   0.854067  0.944444  0.896985    378    21    61  \\\n",
       "\n",
       "   config       model    n  \n",
       "0  cow200  yolo_nas_l  200  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
>>>>>>> 75d00b381e3a5fcb12b0505ee91bac59a8e8cc0a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torchconda)",
   "language": "python",
   "name": "torchconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
