{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mautushid/github/Cowsformer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ultralytics import NAS\n",
    "os.chdir(\"..\")\n",
    "ROOT = os.getcwd()\n",
    "print(ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device to CUDA if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Example of moving a tensor to the chosen device\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The console stream is logged into /Users/mautushid/sg_logs/console.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-07 14:47:13] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n",
      "[2023-12-07 14:47:14] WARNING - __init__.py - Failed to import pytorch_quantization\n",
      "[2023-12-07 14:47:14] WARNING - redirects.py - NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "[2023-12-07 14:47:23] WARNING - calibrator.py - Failed to import pytorch_quantization\n",
      "[2023-12-07 14:47:23] WARNING - export.py - Failed to import pytorch_quantization\n",
      "[2023-12-07 14:47:23] WARNING - selective_quantization_utils.py - Failed to import pytorch_quantization\n",
      "[2023-12-07 14:47:23] WARNING - env_sanity_check.py - \u001b[31mFailed to verify operating system: Deci officially supports only Linux kernels. Some features may not work as expected.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from models.nas import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-07 14:47:27] INFO - checkpoint_utils.py - License Notification: YOLO-NAS pre-trained weights are subjected to the specific license terms and conditions detailed in \n",
      "https://github.com/Deci-AI/super-gradients/blob/master/LICENSE.YOLONAS.md\n",
      "By downloading the pre-trained weight files you agree to comply with these terms.\n",
      "[2023-12-07 14:47:27] INFO - checkpoint_utils.py - Successfully loaded pretrained weights for architecture yolo_nas_l\n"
     ]
    }
   ],
   "source": [
    "### class inits and other inputs\n",
    "\n",
    "path_model = 'yolo_nas_l' \n",
    "dir_train = \"/Users/mautushid/github/Cowsformer/data/cow200/yolov5/train\"\n",
    "dir_val = \"/Users/mautushid/github/Cowsformer/data/cow200/yolov5/val\"\n",
    "dir_test = \"/Users/mautushid/github/Cowsformer/data/cow200/yolov5/test_old\"\n",
    "name_task = \"cow200\"\n",
    "\n",
    "data_yaml_path = \"/Users/mautushid/github/Cowsformer/data/cow200/yolov5/data.yaml\"\n",
    "finetuned_model_path_m = \"/Users/mautushid/github/Cowsformer/lms_checkpoints/n200_yolo_na_i1_exp_yolo_nas_l_200_1/ckpt_best.pth\" \n",
    "                        \n",
    "\n",
    "### Creating instance of Niche_YOLO_NAS class\n",
    "my_nas = Niche_YOLO_NAS(path_model, dir_train, dir_val, dir_test, name_task)\n",
    "#model = my_nas.load(finetuned_model_path_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mautushid/github/Cowsformer/data/cow200/yolov5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-07 14:47:32] INFO - checkpoint_utils.py - Successfully loaded model weights from /Users/mautushid/github/Cowsformer/lms_checkpoints/n200_yolo_na_i1_exp_yolo_nas_l_200_1/ckpt_best.pth EMA checkpoint.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model yolo_nas_l loaded\n"
     ]
    }
   ],
   "source": [
    "best_model = my_nas.load(path_model,finetuned_model_path_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-07 14:35:51] INFO - detection_dataset.py - Dataset Initialization in progress. `cache_annotations=True` causes the process to take longer due to full dataset indexing.\n",
      "Indexing dataset annotations: 100%|██████████| 45/45 [00:00<00:00, 182.02it/s]\n",
      "Testing:   0%|          | 0/3 [00:00<?, ?it/s][W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "Testing: 100%|██████████| 3/3 [01:14<00:00, 21.29s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Precision@0.50': 0.8645319938659668,\n",
       " 'Recall@0.50': 0.9285714030265808,\n",
       " 'mAP@0.50': 0.9126429557800293,\n",
       " 'F1@0.50': 0.8954080939292908,\n",
       " 'Precision@0.50:0.95': 0.6711822748184204,\n",
       " 'Recall@0.50:0.95': 0.7208994626998901,\n",
       " 'mAP@0.50:0.95': 0.6854701042175293,\n",
       " 'F1@0.50:0.95': 0.6951530575752258}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 3/3 [01:24<00:00, 28.11s/it]\n"
     ]
    }
   ],
   "source": [
    "my_nas.evaluate_trained_model(best_model, data_yaml_path, \"test_old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mautushid/miniconda3/envs/myenv/lib/python3.9/site-packages/numpy/lib/arraypad.py:487: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x = np.array(x)\n",
      "/Users/mautushid/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "[2023-12-07 14:47:37] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:37] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:38] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:38] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:39] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:40] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:40] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:41] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:41] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:42] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:42] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:43] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:43] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:44] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:45] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:45] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:46] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:46] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:47] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:47] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:48] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:49] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:49] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:50] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:50] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:51] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:51] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:52] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:53] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:53] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:54] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:54] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:55] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:55] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:56] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:56] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:57] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:58] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:58] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:59] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:47:59] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:48:00] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:48:01] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:48:01] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
      "[2023-12-07 14:48:02] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mAP@50': 0.9491001912194094, 'mAP@50:95': 0.7139276777301194}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_nas.get_map_scores(best_model, data_yaml_path,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store models\n",
    "models_m = []\n",
    "\n",
    "# Loop over each path and load the model\n",
    "for path in finetuned_model_path_m:\n",
    "    model_m = my_nas.load(path_model,path)  # Load the model using the current path\n",
    "    models_m.append(model_m)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_m = []\n",
    "\n",
    "for model in models_m:\n",
    "    metrics_m = my_nas.evaluate_trained_model(model, data_yaml_path, \"test\")  # Replace with your evaluation function\n",
    "    evaluation_results_m.append(metrics_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = 'yolo_nas_l' \n",
    "\n",
    "\n",
    "\n",
    "finetuned_model_path_l = [\"/Users/mautushid/github/Cowsformer/lms_checkpoints/n10_yolo_na_i1_exp_yolo_nas_l_10_1/ckpt_best.pth\", \n",
    "                        \"/Users/mautushid/github/Cowsformer/lms_checkpoints/n25_yolo_na_i1_exp_yolo_nas_l_25_1/ckpt_best.pth\", \n",
    "                        \"/Users/mautushid/github/Cowsformer/lms_checkpoints/n50_yolo_na_i1_exp_yolo_nas_l_50_1/ckpt_best.pth\", \n",
    "                        \"/Users/mautushid/github/Cowsformer/lms_checkpoints/n100_yolo_na_i1_exp_yolo_nas_l_100_1/ckpt_best.pth\",\n",
    "                        \"/Users/mautushid/github/Cowsformer/lms_checkpoints/n200_yolo_na_i1_exp_yolo_nas_l_200_1/ckpt_best.pth\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_l = []\n",
    "\n",
    "# Loop over each path and load the model\n",
    "for path in finetuned_model_path_l:\n",
    "    model_l = my_nas.load(path_model,path)  \n",
    "    models_l.append(model_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_l = []\n",
    "\n",
    "for model in models_l:\n",
    "    metrics_l = my_nas.evaluate_trained_model(model, data_yaml_path, \"test\")  # Replace with your evaluation function\n",
    "    evaluation_results_l.append(metrics_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_l\n",
    "#evaluation_results_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########for the small model ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = 'yolo_nas_s' \n",
    "\n",
    "\n",
    "\n",
    "finetuned_model_path_s = [\"/Users/mautushid/github/Cowsformer/lms_checkpoints/n10_yolo_na_i1_exp_yolo_nas_s_10_1/ckpt_best.pth\", \n",
    "                        \"/Users/mautushid/github/Cowsformer/lms_checkpoints/n25_yolo_na_i1_exp_yolo_nas_s_25_1/ckpt_best.pth\", \n",
    "                        \"/Users/mautushid/github/Cowsformer/lms_checkpoints/n50_yolo_na_i1_exp_yolo_nas_s_50_1/ckpt_best.pth\", \n",
    "                        \"/Users/mautushid/github/Cowsformer/lms_checkpoints/n100_yolo_na_i1_exp_yolo_nas_s_100_1/ckpt_best.pth\",\n",
    "                        \"/Users/mautushid/github/Cowsformer/lms_checkpoints/n200_yolo_na_i1_exp_yolo_nas_s_200_1/ckpt_best.pth\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_s = []\n",
    "\n",
    "# Loop over each path and load the model\n",
    "for path in finetuned_model_path_s:\n",
    "    model_s = my_nas.load(path_model,path)  \n",
    "    models_s.append(model_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_s = []\n",
    "\n",
    "for model in models_s:\n",
    "    metrics_s = my_nas.evaluate_trained_model(model, data_yaml_path, \"test\")  \n",
    "    evaluation_results_s.append(metrics_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_l = pd.DataFrame(evaluation_results_l)\n",
    "df_m = pd.DataFrame(evaluation_results_m)\n",
    "df_s = pd.DataFrame(evaluation_results_s)\n",
    "\n",
    "df_l = pd.DataFrame(evaluation_results_l)\n",
    "df_m = pd.DataFrame(evaluation_results_m)\n",
    "df_s = pd.DataFrame(evaluation_results_s)\n",
    "\n",
    "# Specific model labels\n",
    "model_labels = ['10', '25', '50', '100', '200']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# mAP@0.50 Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(df_l['mAP@0.50'], '-o', label='mAP@0.50 (Large)')\n",
    "plt.plot(df_m['mAP@0.50'], '-s', label='mAP@0.50 (Medium)')\n",
    "plt.plot(df_s['mAP@0.50'], '-^', label='mAP@0.50 (Small)')\n",
    "plt.title('Comparison of mAP@0.50')\n",
    "plt.xlabel('Image Data Size')\n",
    "plt.ylabel('mAP@0.50')\n",
    "plt.xticks(range(len(df_l)), model_labels)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# mAP@0.50:0.95 Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(df_l['mAP@0.50:0.95'], '-o', label='mAP@0.50:0.95 (Large)')\n",
    "plt.plot(df_m['mAP@0.50:0.95'], '-o', label='mAP@0.50:0.95 (Medium)')\n",
    "plt.plot(df_s['mAP@0.50:0.95'], '-o', label='mAP@0.50:0.95 (Small)')\n",
    "plt.title('Comparison of mAP@0.50:0.95')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('mAP@0.50:0.95')\n",
    "plt.xticks(range(len(df_l)), model_labels)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert them to DataFrames\n",
    "df_l = pd.DataFrame(evaluation_results_l)\n",
    "df_m = pd.DataFrame(evaluation_results_m)\n",
    "df_s = pd.DataFrame(evaluation_results_s)\n",
    "\n",
    "# Specific model labels\n",
    "model_labels = ['model_10', 'model_25', 'model_50', 'model_100', 'model_200']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for mAP@0.50\n",
    "plt.plot(df_l['mAP@0.50'], '-o', label='mAP@0.50 (Large)')\n",
    "plt.plot(df_m['mAP@0.50'], '-s', label='mAP@0.50 (Medium)')\n",
    "plt.plot(df_s['mAP@0.50'], '-^', label='mAP@0.50 (Small)')\n",
    "\n",
    "# Plot for mAP@0.50:0.95\n",
    "plt.plot(df_l['mAP@0.50:0.95'], '--o', label='mAP@0.50:0.95 (Large)')\n",
    "plt.plot(df_m['mAP@0.50:0.95'], '--s', label='mAP@0.50:0.95 (Medium)')\n",
    "plt.plot(df_s['mAP@0.50:0.95'], '--^', label='mAP@0.50:0.95 (Small)')\n",
    "\n",
    "# Setting the labels and title\n",
    "plt.title('Comparison of mAP@0.50 and mAP@0.50:0.95')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.xticks(range(len(df_l)), model_labels)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
